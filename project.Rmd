---
title: "BIO 260 Final Project - Yelp Restaurant Success Prediction"
author: "Elena Phuong Dang, Shengyao Jiang, Ming Liu"
output: html_document
---

# Accessing data:
* [Original business data JSON file](https://www.dropbox.com/sh/nnfth7aqy5sfzc3/AACI90MHBGRSEOzqddT0Bsa0a/yelp_academic_dataset_business.json?dl=0).

* [Original review data JSON file](https://www.dropbox.com/sh/nnfth7aqy5sfzc3/AADf6QbvzVK7STvPBlAYXBcQa/yelp_academic_dataset_review.json?dl=0).

* [After-converting business data .RData file](https://www.dropbox.com/s/5ypu3qwfmz7rna3/business.RData?dl=0).

* [After-converting review data .RData file](https://www.dropbox.com/s/6v19tbrjvoo6av2/review.RData?dl=0).

# Motivations

With the development of mobile devices and internet, the effects of social networks has grown increasingly in recent years. We would like to use real world data from social network applications to investigate interesting things in our daily life. We love to explore good food around us in our spare time and used to use Yelp as a searching engine for restaurants. Actually, more and more people use Yelp as a consumer reviews site for finding out top recommendations of restaurants and other services. Thus, we decide to use data from Yelp to predict the success of restaurants. In this project, we included only the restaurants in Pennsylvania (PA) and Quebec (QC) states because we want to compare two states in the North East of America and Canada. We want to study this data and explore the insights of restaurantsâ€™ success near us. 

# Investigating Questions:

* What features are assiocated with the ratings of the restaurants?


    + What kinds of food will determine the success of the restaurants?
  

    + Does the location of states effect the ratings of the restaurants?
  

    + What are the specific amenities that more related to the stars of the restaurants?


    + How does the review text influence the ratings of the restaurants?


* What is the best model to predict the success of the restaurants?

> Let's start our project from data wrangling.

# Data Wrangling

Load our RData files.
```{r,warning=F,message=FALSE}
# set working directory
# Loading RData
library(dplyr)
library(tidyr)

y<-load("business.RData")
rm(y)

r <-load("review.RData")
rm(r)
```

We check the format of each column in the data frame, and we find out that there are lists and data frame inside the data frame of business and review. For example, business categories is the list type, such as c("Chinese","Restaurants"), and attribute is a data frame. For the review data, the votes column is a data frame that consists of votes.funny, votes.useful, and votes.cool. 

```{r}
# Data wrangling
lapply(business, class)
lapply(review,class)
```

### Find all restaurants in QC and PA

Because the business categories is the list, we could not apply the function filter in dplyr, we decided to find all the indexes of restaurants in PA and QC states. Then, we match restaurant index ID with business ID. We subset the business data frame with matching business restaurant ID. For the review data, we find review restaurant ID indexes by the matching business ID. Then, we filtered the review data frame by matching the review ID with the review restaurant indexes. 


```{r}
#table(business$state)

#Filter Restaurants in categories and states PA,QC (states with more than 1000 restaurants)

#getting the index
business_restaurant_index <- which(grepl("Restaurants",business$categories) & business$state %in% c("QC","PA"))

#getting business id by index
business_restaurant_id <- business$business_id[business_restaurant_index]

review_restaurant_index <- which(review$business_id %in% business_restaurant_id) 

review_restaurant_id <- review[review_restaurant_index,"review_id"]

# Filter review by review_id
review <- review[review$review_id %in% review_restaurant_id,]

#Filter business by business_id
business <-business[business$business_id %in% business_restaurant_id,] 
```

### Maps of two states

Before we manipulate more variables, we want to see what the location distributions of the restaurants in this two states. Now we show the restaurants distribution on the maps for PA and QC, and find out that almost all the restaurants in Yelp dataset are around Pittsburgh, the second largest city in Pennsylvania, and Montreal, the largest city in Quebec. We can conclude that there are more restaurants in the big cities, and people are in the large cities would more likely to give the ratings and share their comments on the Yelp.

```{r,warning=F,message=F}
# Map of all restaurants
library(ggmap)
mapsData <- business %>% select(state, longitude, latitude)

mapPA <- get_map("pennsylvania", zoom = 7)
PA <- ggmap(mapPA)+
  geom_point(data = mapsData %>% filter(state == "PA"), aes(x = longitude, y = latitude), color = "#CC6666") 
PA
#ggsave("PA.png", PA)

mapPitts <- get_map("pittsburgh", zoom = 12)
pitts <- ggmap(mapPitts)+
  geom_point(data = mapsData %>% filter(state == "PA"), aes(x = longitude, y = latitude, alpha = 1/20), color = "#CC6666") 
#ggsave("pitts.png", pitts) 
pitts

mapQC <- get_map("quebec", zoom = 5)
QC <- ggmap(mapQC)+
  geom_point(data = mapsData %>% filter(state == "QC"), aes(x = longitude, y = latitude), color = "#6666CC") 
#ggsave("QC.png", QC)
QC

mapMont <- get_map("montreal", zoom = 11)
montreal <- ggmap(mapMont)+
  geom_point(data = mapsData %>% filter(state == "QC"), aes(x = longitude, y = latitude, alpha = 1/20), color = "#6666CC")
#ggsave("montreal.png", montreal)
montreal
```


### Select and manipulate variables

Now we specify some kind of foods and add them into dataset. 

We want to create columns of binary variable for each food cuisine and fast food. The kind of food information is also contained in the business categories column of business data frame. Thus, we have to overcome with the trouble of filtering it. First, we add new columns of Mexican, Fastfood, American,Asian,and European into business data frame. Next, we creat temporary dataframes, and loop through all lists in the business categories column. Then, we use ifelse function to check each condition and assign 1 or 0 to each temporary dataframe. Finally, we replace the empty column with each column of the new dataframes. 

```{r,message=F,warning=F}
# Mexican, fast food, American, Asian, Europeans, Latin Americans

business$Mexican=NA
business$Fastfood=NA
business$American=NA
business$Asian=NA
business$European=NA

ff <- data.frame()
mex <- data.frame()
asia <- data.frame()
america <- data.frame()
european <- data.frame()

for (i in 1:length(business$categories)){
  ff<-rbind(ff,ifelse("Fast Food" %in% business$categories[[i]],1,0))
  mex <- rbind(mex, ifelse("Mexican" %in% business$categories[[i]],1,0))
  asia <- rbind(asia,ifelse("Chinese" %in% business$categories[[i]]|"Japanese" %in% business$categories[[i]]|"Asian Fusion" %in% business$categories[[i]]|"Thai" %in% business$categories[[i]]|"Indian" %in% business$categories[[i]]|"Taiwanese" %in%  business$categories[[i]]|"Vietnamese" %in% business$categories[[i]]|"Korean" %in% business$categories[[i]],1,0))
  america <- rbind(america,ifelse("American (New)" %in% business$categories[[i]]|"American (Traditional)" %in% business$categories[[i]],1,0))
  european <- rbind(european,ifelse("Italian" %in% business$categories[[i]]|"French" %in% business$categories[[i]]|"Irish" %in% business$categories[[i]]|"Greek" %in% business$categories[[i]]|"Austrian" %in% business$categories[[i]]|"British" %in% business$categories[[i]]|"German" %in% business$categories[[i]],1,0))
}

colnames(ff)[1]<- "Fastfood"
colnames(asia)[1]<-"Asia"
colnames(mex)[1] <- "Mexico"
colnames(america)[1] <- "America"
colnames(european)[1] <- "European"

business$Fastfood <- ff$Fastfood
business$Asian <-asia$Asia
business$Mexican <-mex$Mexico
business$American <- america$America
business$European <-european$European

rm(ff)
rm(asia)
rm(mex)
rm(america)
rm(european)
```

Here we add columns of features into the business data frame. 

All features of interest are in the attributes column inside the business data frame. We select reservation, delivery, card, takeout and parking. There are some other potential features and amenities that can affect the star rating. However, they are a lot of missing values, so we decide to filter them out. 

The attribute column is itself a data frame, so we also use similar approach of creating a temporary dataframe and then later replace the column with the empty column of each feature in the business data frame. They are binary variable, and we replace some NA values with zero. 

```{r}
#Attributes: wifi, smoking, reservation, delivery, parking, credit card, happy hour, take out, drive through
business$reservation <- NA
business$delivery <- NA
business$card <- NA
business$takeout <- NA
business$parking <- NA

#save all feature as a seperate data frame
tempA <- data.frame()

#reservation- false 1836, true 2036
tempA <- ifelse(business$attributes$`Takes Reservations`==TRUE,1,0) %>% as.data.frame()
colnames(tempA)[1]<- "reservation"

#delivery- false 2863, true 966
tempA$delivery <- NA
tempA$delivery <- ifelse(business$attributes$Delivery==TRUE,1,0)

#credit card- false 345, true 3792
tempA$card <- NA
tempA$card <- ifelse(business$attributes$`Accepts Credit Cards`==TRUE,1,0)

#take out- false 627, true 3321
tempA$takeout <- NA
tempA$takeout <- ifelse(business$attributes$`Take-out`==TRUE,1,0)

# false 1499, true 1945
tempA$parking <- NA
tempA$parking <- ifelse(business$attributes$Parking$garage==TRUE|business$attributes$Parking$street==TRUE|business$attributes$Parking$validated==TRUE|business$attributes$Parking$lot==TRUE|business$attributes$Parking$valet==TRUE,1,0)

tempA <- tempA %>% replace_na(list(reservation=0,delivery=0,card=0,takeout=0,parking=0))

business$reservation <- tempA$reservation
business$delivery <- tempA$delivery
business$card <- tempA$card
business$takeout <- tempA$takeout
business$parking <- tempA$parking

rm(tempA)
```

Now we add some attributes from review dataset, including number of votes funny, useful, and cool

We use group by and sum up the number of votes funny, useful, and cool of each business ID in the review data frame. We select only columns that contain potential predictor in our model in the business data to delete and clean up other columns of list and data frame type in business. We call the new data frame is business_data. Then, we join the new data frame with columns of votes funny, useful, and cool from the review data.  

```{r, message=F,warning=F}
# select some columns of review and make it not a data.frame
tempB <- review%>%select(business_id,stars)
tempB <- tempB%>%mutate(vote_funny=review$votes$funny,
                        vote_useful=review$votes$useful,
                        vote_cool=review$votes$cool)
#group_by business 
r<-tempB%>%
  group_by(business_id)%>%
  summarise(votes_funny=sum(vote_funny),
            votes_useful=sum(vote_useful), 
            votes_cool=sum(vote_cool))
rm(tempB)

# select columns in business
business_data <- business%>%
  select(business_id,review_count,
         state,stars,reservation,delivery,card,takeout,parking,Mexican,American,Fastfood,  European, Asian)

#left_join the review columns with the business file 
business_data <- left_join(business_data,r,by="business_id")

```

Now we add some text analysis from review data and finalize data wrangling part.

We use the library tidytext to tokenize words from the review text column for each business ID. Then, we also match these word with the sentiments to find positive and negative words. We calculat the positivity of review text based on the number of positive and negative words. Last, we join the column of positivity into the data frame business_data. The positivity is one of the potential predictors in our predictive model. 

We chang the state into binary variable of whether each business ID belongs to the state PA or QC. We delet the business ID column in order to build predictive models with other features. 

```{r}
library(tidytext)

review_text<-review %>%
    select(business_id, text, stars)%>%
    unnest_tokens(word, text)%>%
    filter(!word %in% stop_words$word)

nrc <- sentiments %>%
    filter(lexicon == "nrc") %>%
    select(word, sentiment)

review_sentiment <- review_text %>%
    inner_join(nrc) %>%
     count(business_id,sentiment)%>%
    spread(sentiment, n, fill = 0) %>%
    mutate(positivity = (positive - negative) / (positive + negative + 1))

# join with business_data
# use positivity
business_data <- review_sentiment%>%
  select(c(business_id,positivity))%>%
  left_join(business_data,by="business_id")

# state
business_data<- business_data%>%
  mutate(PA=ifelse(state=="PA",1,0))

business_data <- subset(business_data, select = -c(business_id) )
```  

> Analysis

# Exploratory Analysis (Plots)

We creat plots to explore and visualize the effect of some features on star rating. First, we would like to examine how difference in the text review of star rating of 1 versus star rating of 5. Second, we explore the review count distribution of each star categories for PA versus QC. We also create the density plot to visualize the distribution of positivity of each star category for two states. 

```{r,message=F,warning=F,echo=F}
# exploratory analysis

library(caret)
library(wordcloud)
library(ggplot2)
library(easyGgplot2)

#word cloud by filtered 1-star yelp review/ 5-star yelp review (in review data)

common_1star <- review_text %>%
                  filter(stars==1) %>% count(word, sort = TRUE) %>% filter(n > 450)
pal1 <- brewer.pal(5, "RdPu")
pal1 <- pal1[-(1:2)]
wordcloud(common_1star$word, common_1star$n, colors = pal1, scale=c(12,.3))

common_5star <- review_text %>%
                  filter(stars==5) %>% count(word, sort = TRUE) %>% filter(n > 1000)
pal2 <- brewer.pal(5, "YlGn")
pal2 <- pal2[-(1:2)]
wordcloud(common_5star$word, common_5star$n, colors = pal2, scale=c(12,.3))


#review_count vs. stars/ positivity vs. stars
ggplot2.histogram(data=business_data, xName='positivity',
        groupName='stars', legendPosition="top",
        alpha=0.5, addDensityCurve=TRUE, scale=density) + facet_wrap(~state)

ggplot2.barplot(data=business_data, xName='stars',yName='log10(review_count)', groupName = 'stars') + facet_wrap(~state)

```


# Cross validation

We divide the data into training and testing sets randomly, with 80% of the data for training and 20% for testing. We use the training set to build the predictive models and evaluate the model with the testing set. 

```{r}
# cross validation
#library(caret)
#delete the orginal state column
business_data <- business_data %>% select(-c(state))

# create train set and test set with 80% in train and 20% in test
inTrain <- createDataPartition(y = business_data$stars, p=0.8)
training <- slice(business_data, inTrain$Resample1)
testing <- slice(business_data, -inTrain$Resample1)

#run 10 cross validations leaving out 20% of the data
control <- trainControl(method='cv', number=10, p=.8) 
```

# Fitting different models

### Linear model

We first build the linear regression with all potential predictors, and we use backward selection to choose significant features that related to star rating. These features will be used to build other prediction models to figure out the most effective model for the prediction of star rating of restaurants. 

```{r}
res_lm1 <- training%>%
  lm(stars ~ ., data = .)

summary(res_lm1)

#remove European
res_lm2 <- training%>%
  lm(stars ~ positivity + review_count + reservation + delivery + card +  takeout + parking+ Mexican + American + Fastfood + Asian + votes_funny + votes_useful + votes_cool + PA, data = .)

summary(res_lm2) 

#remove PA
res_lm3 <- training%>%
  lm(stars ~ positivity + review_count + reservation + delivery + card +  takeout + parking+ Mexican + American + Fastfood + Asian + votes_funny + votes_useful + votes_cool, data = .)

summary(res_lm3) 

#remove Mexican
res_lm4 <- training%>%
  lm(stars ~ positivity + review_count + reservation + delivery + card +  takeout + parking+ American + Fastfood + Asian + votes_funny + votes_useful + votes_cool, data = .)

summary(res_lm4) 

#remove Reservation
res_lm5 <- training%>%
  lm(stars ~ positivity + review_count + delivery + card +  takeout + parking+ American + Fastfood + Asian + votes_funny + votes_useful + votes_cool, data = .)

summary(res_lm5) 

#RMSE = SD/(1-R^2)
rmse_lm <- 0.6114*sqrt(1-0.3027) #0.51

```

Based on the result of the backward selection in linear regression model, we leave out four variables. They are European, PA, Mexican, and reservation. 

### GLM (Logistic Regression)

We fit other models using the selected variables from the result of backward selection. The first model is logistic regression. 

```{r,message=F,warning=F}
# GLM (Logistic Regression)

fit1 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==1),family="binomial")
fit2 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==1.5),family="binomial")
fit3 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==2),family="binomial")
fit4 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==2.5),family="binomial")
fit5<- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==3),family="binomial")
fit6 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==3.5),family="binomial")
fit7 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==4),family="binomial")
fit8 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==4.5),family="binomial")
fit9 <- glm(y~., data=mutate(training %>% select(-c(European, PA, Mexican, reservation)),
                                   y=stars==5),family="binomial")


f_hat1 <- predict(fit1, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type = "response")
f_hat2 <- predict(fit2, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type ="response")
f_hat3 <- predict(fit3, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type = "response")
f_hat4 <- predict(fit4, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type ="response")
f_hat5 <- predict(fit5, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type = "response")
f_hat6 <- predict(fit6, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type ="response")
f_hat7 <- predict(fit7, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type = "response")
f_hat8 <- predict(fit8, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type ="response")
f_hat9 <- predict(fit9, newdata = testing %>% select(-c(European, PA, Mexican, reservation)), type = "response")
 

pred_glm <- 
  apply(cbind(f_hat1, f_hat2, f_hat3,f_hat4,f_hat5,f_hat6,f_hat7,f_hat8,f_hat9),1,which.max)
```

### KNN

Another model that we try is the K Nearest Neighbors model. We use cross validation to select the best k first, and we find out that k=19 is the best value. We fit the model using k=19. 

_Due to the random spliting training dataset and testing dataset, the best K value would be changed every time. Here we just use the K=19._

```{r}
#KNN
res <- train(as.factor(stars) ~ .,
             data = training %>% select(-c(European, PA, Mexican, reservation)),
             method = "knn",
             trControl = control,
             tuneLength = 1, 
             tuneGrid=data.frame(k=seq(1,20,2)),
             metric="Accuracy")

plot(res)

res$results %>% 
  ggplot(aes(k, Accuracy, ymin= Accuracy - AccuracySD, 
             ymax = Accuracy + AccuracySD)) +
  geom_point() + geom_errorbar()
res
# fit with k=19
fit <- knn3(stars~., data=training %>% select(-c(European, PA, Mexican, reservation)), k=19)
f_hat <- predict(fit, newdata = testing %>% select(-c(European, PA, Mexican, reservation)))
pred_knn_19 <- apply(f_hat,1,which.max)
```


### SVM and Random Forest

We would also like to try to build predictive model with support vector machine and random forest. 

```{r,message=F,warning=F}
#SVM
#install.packages("e1071")

library(e1071)
fit_svm <- svm(stars~.,data=training %>% select(-c(European, PA, Mexican, reservation))) 
pred_svm <-predict(fit_svm,newdata=testing %>% select(-c(European, PA, Mexican, reservation)))

# Random Forest
library(randomForest)
fit_rr<-randomForest(stars~.,data=training %>% select(-c(European, PA, Mexican, reservation)))
pred_rr<-predict(fit_rr,newdata=testing %>% select(-c(European, PA, Mexican, reservation)))
```

#RMSE

For comparing these models we fitted above and finding out the best one, we calculate the root mean squared error of the predicted star rating and the true rating for each model. 

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

rmse_glm <- RMSE(testing$stars,pred_glm)
rmse_glm 

rmse_knn_19<- RMSE(testing$stars,pred_knn_19)
rmse_knn_19 

rmse_svm <- RMSE(testing$stars,pred_svm)
rmse_svm 

rmse_rr <- RMSE(testing$stars,pred_rr)
rmse_rr 
```

#Table of RMSE Comparison

We create the table of RMSE for each model to compare the result. The logistic regression and k-Nearest neighbors do not perform very well. The RMSE of random forest and support vector machine values are smaller. Apparently the linear model has the smallest RMSE and would be our best model. 

```{r}
library(knitr)
rmse_results <- data_frame(method = "Logistic Regression", RMSE = rmse_glm)

rmse_results <- bind_rows(rmse_results, data_frame(method = "Linear Regression", RMSE = rmse_lm))

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="K Nearest Neighbors",  
                                     RMSE = rmse_knn_19 ))

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Support Vector Machine",  
                                     RMSE = rmse_svm ))

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Random Forest",  
                                     RMSE = rmse_rr ))


rmse_results%>%kable
```

# Summary and Conclusions

From the project we could answer the questions we wanted to investigate.

* What features are assiocated with the ratings of the restaurants?

_We found out that delivery service, take-out, parking, accepting credit cards or not, American food, fast food, Asian food, the positivity of the review text, the counts of the reviews, and the number of the received votes of "useful", "funny" and "cool" from other users have more effect on the stars of the restaurants._ 

* What kinds of food will determine the success of the restaurants?

_American food, fast food, and Asian food are more related to the success of the restaurants than other kinds of food._

* Does the location of states effect the ratings of the restaurants?

_Not really._

* What are the specific amenities that more related to the stars of the restaurants?

_Guests would more like to rate them if they have delivery, take-out, parking, accepting credit cards services._

* How does the review text influence the ratings of the restaurants?

_The more positivity of the review text, the higher ratings of the restaurants._

* What is the best model to predict the success of the restaurants?

_By comparing the RMSE, we found out that the linear model outperforms other models._
